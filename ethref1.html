<html>
<head>
  <meta charset="utf-8" >
  <style>
  @import 'https://fonts.googleapis.com/css?family=Raleway:300,400,500';
  a {
    color: #ff0800
  }
  .headers {
    text-shadow: 1px 0.7px rgb(80,81,79);
    font-family: "Raleway";
    font-size: 30px;
  }
  h1 {
    font-size: 40px;
    font-family: "Raleway";
    text-shadow: 1px 0.7px rgb(0, 15, 28);
  }
  div {
    color:#000F1C;
    font-family: georgia;
    width: 60%;
    margin: 10 auto;
    background-color: rgba(#e7bb9b, 0.65);
    border-radius: 10px;
    padding-top: 30px;
    padding-bottom: 30px;
    padding-left: 20px;
    padding-right: 20px;
    line-height: 1.75;
  }
  body {
  padding: 20px;
  font-family: georgia;
  background-color: #FAFAC6;
}
</style>
</head>
<body>
  <div>
<h1 style="text-align:center">Ethical Reflection One - Machine Bias </h1>
</div>
<div>
  <h2><strong>7/11/18</strong></h2>
  <p>Sources used: Propublica's <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">Machine Bias</a> and MIT Technology Review's <a href="https://www.technologyreview.com/s/607955/inspecting-algorithms-for-bias/">Inspecting Algorithms for Bias</a></p>
  <p>The use of computer algorithms to inform criminal courts on the likelihood of a criminal to re-offend has started a debate on the validity of a questionnaire in a court of law and whether or not they are or can be useful. These algorithms go on to inform a court on how harsh a defendant’s treatment should be in the justice system and lead to decisions like likelihood of parole. ProPublica conducted research to see if the use of these algorithms disproportionately hurt black people when compared to white people. What was found was that a such algorithm, COMPAS, could accurately predict white and black offenders’ likelihood of reoffending at the same rate but would more often give a harsher score to a black person that would not go on to reoffend than give such a score to a white person, as stated in the MIT article. The ProPublica article gives many stories of people with similar crimes who got very different numbers, with the white criminal having a less harsh score than a black criminal. One idea that is talked about in both articles is how the algorithms inner workings are a mystery to most. Seeing how it can be used to lead to a person being unnecessarily punished, many see the use of these algorithms to be problematic if the defendant does not know what goes in to producing this score.
  </p>
  <p>In response, MIT Technology Review has pointed at the idea of bettering the code rather than completely casting it aside from the court room. Though it does have issues, giving false flags and leading to a tougher situation for some, more needs to be put into this algorithm before it can be truly useful. MIT pointed out how human judges can be unintentionally biased, giving the famous example of a study that concluded that judges on parole boards would give softer judgement on criminals if they had a break for food. In my opinion, the use and inner workings of these algorithms should be more transparent to the defendants and should be used sparingly until the algorithms are shown to be useful. One way of studying this could be by feeding the algorithms information on defendants and comparing this information with whether or not the criminals actually went on to reoffend, tweaking the algorithm with this data and use it in court rooms after it has been shown to be effective. This technology could prove to help lessen the number of instances of bias in the justice system as well as allow for fairer sentencing, and it would be a shame to lose such a technology.
  </p>
</div>
</body>
</html>
